#TOOLBOX lives at the TOP LEVEL of the chart values
cephImage:
  repository: quay.io/ceph/ceph
  tag: v18.2.7

toolbox:
  enabled: true
  image: quay.io/ceph/ceph:v18.2.7

cephClusterSpec:
  dataDirHostPath: /var/lib/rook
  
  cephVersion:
    image: quay.io/ceph/ceph:v18.2.7
    allowUnsupported: false

  mon:
    count: 3
    allowMultiplePerNode: false

  mgr:
    count: 1

  dashboard:
    enabled: true
    ssl: false

  # Run OSDs only on labeled nodes
  placement:
    all:
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
        - key: node-role.kubernetes.io/master
          operator: Exists
          effect: NoSchedule
    osd:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-osd
              operator: In
              values: ["enabled"]
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
        - key: node-role.kubernetes.io/master
          operator: Exists
          effect: NoSchedule

  storage:
    useAllNodes: true
    useAllDevices: false
    deviceFilter: '^(sd[b-z]|vd[b-z]|nvme\d+n\d+)$'   # keep empty unless you want to match /dev names by regex
    # You can explicitly list devices per node instead of deviceFilter, e.g.:
    # nodes:
    # - name: worker-1
    #   devices: [{name: "/dev/sdb"}]
    config:
      osdsPerDevice: "1"
  
  resources:
    osd:
      requests:
        cpu: "250m"
        memory: "768Mi"
    prepareosd:
      requests:
        cpu: "100m"
        memory: "256Mi"

  network:
    provider: host

# Create a default RBD StorageClass right away
cephBlockPools:
  - name: replicapool
    spec:
      failureDomain: host
      replicated:
        size: 3             # <-- change to 2 if you only have 2 OSD nodes
        requireSafeReplicaSize: true
      parameters:
        compression_mode: "none"
    storageClass:
      enabled: true
      name: rook-ceph-block
      isDefault: true
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: Immediate
      parameters:
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        imageFeatures: layering

# (Optional) you can add a CephFS SC later if needed
cephFileSystems: []

