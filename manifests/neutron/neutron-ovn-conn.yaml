apiVersion: v1
kind: ServiceAccount
metadata:
  name: neutron-ovn-conn-writer
  namespace: openstack
  labels:
    app.kubernetes.io/name: neutron-ovn-conn
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: neutron-ovn-conn-writer
  namespace: openstack
rules:
  - apiGroups: [""]
    resources: ["services"]
    verbs: ["get","list","watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get","create","update","patch","list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: neutron-ovn-conn-writer
  namespace: openstack
subjects:
  - kind: ServiceAccount
    name: neutron-ovn-conn-writer
    namespace: openstack
roleRef:
  kind: Role
  name: neutron-ovn-conn-writer
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: batch/v1
kind: Job
metadata:
  name: neutron-ovn-conn
  namespace: openstack
  labels:
    app.kubernetes.io/name: neutron-ovn-conn
  # If you use Argo sync waves, put an annotation like this on the object
  # so it runs before neutron (e.g., wave 35 if neutron is 40).
  annotations:
    argocd.argoproj.io/sync-wave: "35"
spec:
  backoffLimit: 3
  template:
    metadata:
      labels:
        app.kubernetes.io/name: neutron-ovn-conn
    spec:
      serviceAccountName: neutron-ovn-conn-writer
      restartPolicy: OnFailure
      containers:
        - name: writer
          image: bitnami/kubectl:1.30
          imagePullPolicy: IfNotPresent
          command: ["/bin/bash","-c"]
          env:
            - name: NS
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          args:
            - |
              set -euo pipefail

              echo "[INFO] Discovering OVN OVSDB services in namespace: ${NS}"

              # Prefer ClusterIP IPv4; fallback to first ClusterIP if multiple
              get_ip () {
                local svc="$1"
                # Try spec.clusterIPs[0], fallback to spec.clusterIP
                kubectl -n "${NS}" get svc "${svc}" -o json \
                  | jq -r 'if .spec.clusterIPs then .spec.clusterIPs[0] else .spec.clusterIP end'
              }

              NB_SVC="ovn-ovsdb-nb"
              SB_SVC="ovn-ovsdb-sb"
              NB_IP="$(get_ip "${NB_SVC}")"
              SB_IP="$(get_ip "${SB_SVC}")"

              if [[ -z "${NB_IP}" || "${NB_IP}" == "null" ]]; then
                echo "[ERROR] Could not resolve ClusterIP for ${NB_SVC}"; exit 1
              fi
              if [[ -z "${SB_IP}" || "${SB_IP}" == "null" ]]; then
                echo "[ERROR] Could not resolve ClusterIP for ${SB_SVC}"; exit 1
              fi

              # Default OVN ports used by your setup
              NB_PORT="${NB_PORT:-6641}"
              SB_PORT="${SB_PORT:-6642}"

              echo "[INFO] Using NB tcp://${NB_IP}:${NB_PORT}  SB tcp://${SB_IP}:${SB_PORT}"

              cat > /work/99-ovn-conn.ini <<EOF
              [ovn]
              ovn_nb_connection = tcp:${NB_IP}:${NB_PORT}
              ovn_sb_connection = tcp:${SB_IP}:${SB_PORT}
              neutron_sync_mode = repair
              EOF

              # Idempotent apply of a dedicated ConfigMap (does not fight Helm’s chart-managed configmaps)
              kubectl -n "${NS}" create configmap neutron-ovn-conn \
                --from-file=99-ovn-conn.ini=/work/99-ovn-conn.ini \
                --dry-run=client -o yaml \
              | kubectl -n "${NS}" apply -f -

              echo "[INFO] ConfigMap neutron-ovn-conn updated."

              # Optional: verify DB reachability now to fail fast (non-fatal if you’d rather just create the CM)
              if command -v ovsdb-client >/dev/null 2>&1; then
                echo "[INFO] ovsdb-client present in image; verifying NB connectivity..."
                ovsdb-client -t 3 list-dbs "tcp:${NB_IP}:${NB_PORT}" >/dev/null && echo "[INFO] NB reachable."
              else
                echo "[WARN] ovsdb-client not present in this image; skipping reachability check."
              fi

              echo "[INFO] Done."
          volumeMounts:
            - name: work
              mountPath: /work
      volumes:
        - name: work
          emptyDir: {}
