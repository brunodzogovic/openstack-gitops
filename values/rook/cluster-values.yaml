cephClusterSpec:
  dataDirHostPath: /var/lib/rook
  cephVersion:
    image: quay.io/ceph/ceph:v18.2.2
    allowUnsupported: false

  mon:
    count: 3
    allowMultiplePerNode: false

  mgr:
    count: 1

  dashboard:
    enabled: true
    ssl: false

  # Run OSDs only on labeled nodes
  placement:
    osd:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: ceph-osd
              operator: In
              values: ["enabled"]

  storage:
    useAllNodes: false
    useAllDevices: false
    deviceFilter: ""        # keep empty unless you want to match /dev names by regex
    # You can explicitly list devices per node instead of deviceFilter, e.g.:
    # nodes:
    # - name: worker-1
    #   devices: [{name: "/dev/sdb"}]
    config:
      osdsPerDevice: "1"

  network:
    provider: host

# Create a default RBD StorageClass right away
cephBlockPools:
  - name: replicapool
    spec:
      failureDomain: host
      replicated:
        size: 3             # <-- change to 2 if you only have 2 OSD nodes
    storageClass:
      enabled: true
      name: rook-ceph-block
      isDefault: true
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: Immediate
      parameters:
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        imageFeatures: layering

# (Optional) you can add a CephFS SC later if needed
cephFileSystems: []

